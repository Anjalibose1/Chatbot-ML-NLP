import json
import random
import pickle
import numpy as np
import nltk
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# Download NLTK data
nltk.download("punkt")
nltk.download("wordnet")

# Load JSON data
with open("training_data.json", "r") as file:
    data = json.load(file)

lemmatizer = WordNetLemmatizer()
all_words = []
tags = []
xy = []

# Prepare data
for intent in data["intents"]:
    tag = intent["tag"]
    tags.append(tag)
    for pattern in intent["patterns"]:
        words = nltk.word_tokenize(pattern)
        words = [lemmatizer.lemmatize(w.lower()) for w in words]
        all_words.extend(words)
        xy.append((words, tag))

# Sort and deduplicate
all_words = sorted(set(all_words))
tags = sorted(set(tags))

# Create training data
X_train = []
y_train = []

vectorizer = CountVectorizer(v
